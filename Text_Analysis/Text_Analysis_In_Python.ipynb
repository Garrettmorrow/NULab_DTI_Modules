{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis Using Python\n",
    "\n",
    "In this tutorial we will be using Python to perform computational text analysis on a corpus of our choosing. While many of these functions are available in free web tools like WordCounter, SameDiff, and Voyant among others, using Python will allow for more advanced functions like natural language processing, character-space network modeling, vector space analysis, and topic modeling. This tutorial is structued into multiple exercises that gradually get more complex.\n",
    "\n",
    "## Module Outline\n",
    "\n",
    "1. Word Counts and Descriptive Statistics\n",
    "2. To be continued...\n",
    "\n",
    "## Data\n",
    "For much of this tutorial we will be using data from \"The American Presidency Project\" of the University of California Santa Barbara, available at: https://www.presidency.ucsb.edu/.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## Contact Information\n",
    "Garrett Morrow <br>\n",
    "Digital Teaching Integration Research Fellow <br>\n",
    "PhD Student, Political Science <br>\n",
    "morrow.g [at] husky [dot] neu [dot] edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do:\n",
    "1. Add in more detailed documentation to Exercise 1 code.\n",
    "2. Adapt Natural Language Processing from Prof. Nelson's tutorial.\n",
    "3. Adapt Vector Space modeling from Prof. Nelson's tutorial.\n",
    "4. Find topic modeling resources to create new exercise.\n",
    "5. Find character space / network modeling resources to create new exercise.\n",
    "6. Add an exercise in sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 1\n",
    "## Word Counts and Descriptive Statistics\n",
    "The first exercise is a basic tutorial that uses Python to read in .txt files, and finds descriptive statistics about the texts. We will find character and word counts, average word lengths, token counts (aka unique words), token ratios, number of gerunds (action words), and gerund ratio.\n",
    "\n",
    "## Data\n",
    "For this exercise, we are comparing six text documents from \"The American Presidency Project\". Four of them are recent platforms, and for fun, two are more historical (Richard Nixon's from 1968 and Abraham Lincoln's from 1960):\n",
    "1. 2016 Democratic Party Political Platform (Hillary Clinton)\n",
    "2. 2016 Republican Party Political Platform (Donald Trump)\n",
    "3. 2012 Democratic Party Political Platform (Barack Obama)\n",
    "4. 2012 Republican Party Political Platform (Mitt Romney)\n",
    "5. 1968 Republican Party Political Platform (Richard Nixon)\n",
    "6. 1860 Republican Party Political Platform (Abraham Lincoln)\n",
    "\n",
    "## Libraries Used:\n",
    "1. String\n",
    "2. Matplotlib (specifically the pyplot library)\n",
    "3. Pandas\n",
    "\n",
    "## Key Terminology\n",
    "<b> nGram:</b> <br>\n",
    "<b> Corpus:</b> <br>\n",
    "<b> Stopwords:</b> <br>\n",
    "<b> Term Frequency:</b> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: Reading in and Cleaning Data (aka Pre-Processing)\n",
    "The first steps for Python text analysis is to read in data and clean it by making all the text lowercase and removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines load in our required Python libraries.\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to read in our data files.\n",
    "\n",
    "def read_file(name):\n",
    "    with open(name, 'r', encoding = 'utf-8') as my_file:\n",
    "        return my_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use the function we just created to read in the files and assign them to variables.\n",
    "\n",
    "clinton = read_file('data/2016clintonH.txt')\n",
    "trump = read_file('data/2016trump.txt')\n",
    "obama = read_file('data/2012obama2.txt')\n",
    "romney = read_file('data/2012romney.txt')\n",
    "nixon = read_file('data/1968nixon.txt')\n",
    "lincoln = read_file('data/1860lincoln.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will clean our data by spliting our strings on spaces, \n",
    "# making all the text lowercase, and removing punctuation.\n",
    "\n",
    "def cleaner(file):\n",
    "    newlist = []\n",
    "    for word in file.split():\n",
    "        word = word.lower()\n",
    "        word = ''.join([ch for ch in word if ch not in string.punctuation])\n",
    "        newlist.append(word)\n",
    "    return newlist\n",
    "\n",
    "# These lines clean our read text strings and assigns them to variables.\n",
    "\n",
    "clinton_clean = cleaner(clinton)\n",
    "trump_clean = cleaner(trump)\n",
    "obama_clean = cleaner(obama)\n",
    "romney_clean = cleaner(romney)\n",
    "nixon_clean = cleaner(nixon)\n",
    "lincoln_clean = cleaner(lincoln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Calculating Descriptive Statistics\n",
    "Now that we have clean lists of our texts, we can calculate descriptive statistics so we can then put them into dataframes using the 'pandas' library, and then visualize our statistics using 'pyplot' that is a part of 'matplotlib' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will count individual characters (not including spaces and punctuation).\n",
    "\n",
    "def characters(file):\n",
    "    char_count = 0\n",
    "    for word in file:\n",
    "        for letter in word:\n",
    "            char_count += 1\n",
    "    return char_count\n",
    "\n",
    "# We then run our function on our cleaned text to count characters.\n",
    "\n",
    "clinton_chars = characters(clinton_clean)\n",
    "trump_chars = characters(trump_clean)\n",
    "obama_chars = characters(obama_clean)\n",
    "romney_chars = characters(romney_clean)\n",
    "nixon_chars = characters(nixon_clean)\n",
    "lincoln_chars = characters(lincoln_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines very simply count total words similar to the above function counting characters,\n",
    "# but by counting the length of the clean text lists..\n",
    "\n",
    "clinton_words = len(clinton_clean)\n",
    "trump_words = len(trump_clean)\n",
    "obama_words = len(obama_clean)\n",
    "romney_words = len(romney_clean)\n",
    "nixon_words = len(nixon_clean)\n",
    "lincoln_words = len(lincoln_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines calculate the average characters per word - a possible indicator of text complexity.\n",
    "\n",
    "clinton_average = clinton_chars / clinton_words\n",
    "trump_average = trump_chars / trump_words\n",
    "obama_average = obama_chars / obama_words\n",
    "romney_average = romney_chars / romney_words\n",
    "nixon_average = nixon_chars / nixon_words\n",
    "lincoln_average = lincoln_chars / lincoln_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates tokens, or unique words - another possible indicator of complexity.\n",
    "\n",
    "def tokens(file):\n",
    "    newlist = []\n",
    "    for word in file:\n",
    "        if word not in newlist:\n",
    "            newlist.append(word)\n",
    "    return newlist\n",
    "\n",
    "clinton_token_count = len(tokens(clinton_clean))\n",
    "trump_token_count = len(tokens(trump_clean))\n",
    "obama_token_count = len(tokens(obama_clean))\n",
    "romney_token_count = len(tokens(romney_clean))\n",
    "nixon_token_count = len(tokens(nixon_clean))\n",
    "lincoln_token_count = len(tokens(lincoln_clean))\n",
    "\n",
    "# The texts in our corpus are different lengths, so this function calculates the token ratio to somewhat normalize them.\n",
    "\n",
    "clinton_token_ratio = clinton_token_count / clinton_words\n",
    "trump_token_ratio = trump_token_count / trump_words\n",
    "obama_token_ratio = obama_token_count / obama_words\n",
    "romney_token_ratio = romney_token_count / romney_words\n",
    "nixon_token_ratio = nixon_token_count / nixon_words\n",
    "lincoln_token_ratio = lincoln_token_count / lincoln_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates gerunds (words ending in -ing), a rough estimate of actionable content.\n",
    "\n",
    "def gerunds(file):\n",
    "    gerund_count = 0\n",
    "    for word in file:\n",
    "            if word[-3:] == 'ing':\n",
    "                gerund_count += 1\n",
    "    return gerund_count\n",
    "\n",
    "clinton_gerunds = gerunds(clinton_clean)\n",
    "trump_gerunds = gerunds(trump_clean)\n",
    "obama_gerunds = gerunds(obama_clean)\n",
    "romney_gerunds = gerunds(romney_clean)\n",
    "nixon_gerunds = gerunds(nixon_clean)\n",
    "lincoln_gerunds = gerunds(lincoln_clean)\n",
    "\n",
    "# We also calculate the gerund ratio of each text similar to token ratios.\n",
    "\n",
    "clinton_gerund_ratio = clinton_gerunds / clinton_words\n",
    "trump_gerund_ratio = trump_gerunds / trump_words\n",
    "obama_gerund_ratio = obama_gerunds / obama_words\n",
    "romney_gerund_ratio = romney_gerunds / romney_words\n",
    "nixon_gerund_ratio = nixon_gerunds / nixon_words\n",
    "lincoln_gerund_ratio = lincoln_gerunds / lincoln_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Three: Pandas!\n",
    "In this step we will assemble our data into a 'pandas' dataframe for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Total_Characters</th>\n",
       "      <th>Total_Words</th>\n",
       "      <th>Average_Characters/Word</th>\n",
       "      <th>Total_Tokens</th>\n",
       "      <th>Token_Ratio</th>\n",
       "      <th>Total_Gerunds</th>\n",
       "      <th>Gerund_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>140443</td>\n",
       "      <td>25966</td>\n",
       "      <td>5.408727</td>\n",
       "      <td>4026</td>\n",
       "      <td>0.155049</td>\n",
       "      <td>1034</td>\n",
       "      <td>0.039821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>191177</td>\n",
       "      <td>35621</td>\n",
       "      <td>5.366975</td>\n",
       "      <td>5476</td>\n",
       "      <td>0.153730</td>\n",
       "      <td>1002</td>\n",
       "      <td>0.028129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>139725</td>\n",
       "      <td>26558</td>\n",
       "      <td>5.261127</td>\n",
       "      <td>3795</td>\n",
       "      <td>0.142895</td>\n",
       "      <td>1103</td>\n",
       "      <td>0.041532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>165358</td>\n",
       "      <td>30541</td>\n",
       "      <td>5.414296</td>\n",
       "      <td>5005</td>\n",
       "      <td>0.163878</td>\n",
       "      <td>924</td>\n",
       "      <td>0.030254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard Nixon</td>\n",
       "      <td>55487</td>\n",
       "      <td>9956</td>\n",
       "      <td>5.573222</td>\n",
       "      <td>2513</td>\n",
       "      <td>0.252411</td>\n",
       "      <td>271</td>\n",
       "      <td>0.027220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>6190</td>\n",
       "      <td>1194</td>\n",
       "      <td>5.184255</td>\n",
       "      <td>517</td>\n",
       "      <td>0.432998</td>\n",
       "      <td>19</td>\n",
       "      <td>0.015913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Candidate  Total_Characters  Total_Words  Average_Characters/Word  \\\n",
       "0  Hillary Clinton            140443        25966                 5.408727   \n",
       "1     Donald Trump            191177        35621                 5.366975   \n",
       "2     Barack Obama            139725        26558                 5.261127   \n",
       "3      Mitt Romney            165358        30541                 5.414296   \n",
       "4    Richard Nixon             55487         9956                 5.573222   \n",
       "5  Abraham Lincoln              6190         1194                 5.184255   \n",
       "\n",
       "   Total_Tokens  Token_Ratio  Total_Gerunds  Gerund_Ratio  \n",
       "0          4026     0.155049           1034      0.039821  \n",
       "1          5476     0.153730           1002      0.028129  \n",
       "2          3795     0.142895           1103      0.041532  \n",
       "3          5005     0.163878            924      0.030254  \n",
       "4          2513     0.252411            271      0.027220  \n",
       "5           517     0.432998             19      0.015913  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will create a pandas dataframe by hand with our variables, to do this we first assign our multiple variables\n",
    "# to a dictionary where the values are lists.\n",
    "\n",
    "data = {'Candidate': ['Hillary Clinton', 'Donald Trump', 'Barack Obama', 'Mitt Romney', 'Richard Nixon', 'Abraham Lincoln'], \n",
    "        'Total_Characters': [clinton_chars, trump_chars, obama_chars, romney_chars, nixon_chars, lincoln_chars],\n",
    "        'Total_Words': [clinton_words, trump_words, obama_words, romney_words, nixon_words, lincoln_words],\n",
    "        'Average_Characters/Word': [clinton_average, trump_average, obama_average, romney_average, nixon_average, lincoln_average],\n",
    "        'Total_Tokens': [clinton_token_count, trump_token_count, obama_token_count, romney_token_count, nixon_token_count, lincoln_token_count],\n",
    "        'Token_Ratio': [clinton_token_ratio, trump_token_ratio, obama_token_ratio, romney_token_ratio, nixon_token_ratio, lincoln_token_ratio],\n",
    "        'Total_Gerunds': [clinton_gerunds, trump_gerunds, obama_gerunds, romney_gerunds, nixon_gerunds, lincoln_gerunds],\n",
    "        'Gerund_Ratio': [clinton_gerund_ratio, trump_gerund_ratio, obama_gerund_ratio, romney_gerund_ratio, nixon_gerund_ratio, lincoln_gerund_ratio]\n",
    "       }\n",
    "\n",
    "# Then we form our dataframe and name our columns.\n",
    "\n",
    "df = pandas.DataFrame(data, columns = ['Candidate', \n",
    "                                       'Total_Characters', \n",
    "                                       'Total_Words', \n",
    "                                       'Average_Characters/Word',\n",
    "                                       'Total_Tokens',\n",
    "                                       'Token_Ratio', \n",
    "                                       'Total_Gerunds', \n",
    "                                       'Gerund_Ratio'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Four: Visualization\n",
    "We now will use 'pyplot', a part of 'matplotlib', to visualize some of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm4HEW9//H3x4R9C8uBiyQQhCgCSoAQUJSLgBBABRQERDb1RgRExOWCPzWKgFwFUS6LIkQSRBFBJEq4IbKKbAkQE8IiIQQICSGQhSBrwvf3R9WQzjAzZ07Sc07Oyef1PPNMT3V1dXVPT3+7q3uqFRGYmZmV6V1dXQEzM+t5HFzMzKx0Di5mZlY6BxczMyudg4uZmZXOwcXMzErn4GJNkbSqpJDUt6vrAiDpOEl/W4rp7pH0+VbUqRUk7SXpnw3GXyXpuy2a9w8lXdBk3pbVo6OKdenK9beic3DpxiS9XHi9JenVwucj2pl2iKQpJdXjWEkPVKX9vU7ayWXMs6tJ2ioH28r6nirpG2XPJyL+FhHblV1utVrbQ0QMi4gTSyq/r6QRkmZJeknSw5K+J2nVMsqvp8z1190OTLqag0s3FhFrVl7A08AnC2lXdmJV7gC2k7QOpLMc4H1AW1XaTjlvh0jqVWJdy7SosP6PAc6StHt1Jkm9O7tiyxNJGwL3AAHsFBFrA/sBGwObdWXdrHUcXHowSatJulDSTEnTJf1U0kqS1geuA95TOPJeX9Kuku6VNF/SDEnnNbNjjIgngJnAR3LSYOB+4K6qtNeBf+a6fSCfycyTNFHSvoV6XyXpfEk3Sfo38CFJG0oanY9676awU5LUS9IFkmbnuv9T0vsaVPl9ku7Pea8tBMCbJf1X1Tr8l6QhTayDO4B/AdsWmhC/IukJ4KFc1raSbpE0V9Ijkg4szOcASY9KWiDpGUkn5fQlzigkDc7Lt0DSb4GVq+p7UF6f8/L63bow7jlJX5f0UF72KyWt3GB7OFvSpXna3nldzcpl39rOOi76NvAccGxEPJ3X17SIOD4iHsvlX5y30Zck3Sdpl0K9z851/X1e7omSBjazTjqy/iS1Sboxb0dzJF0vaeM87lzSwdGlef2c2953uqJzcOnZfgh8EPgAsCOwO/DtiHgROAiYWjjTeRF4EzgRWA/4KPBJ4EtNzuvvwG55eLf8+c6qtH9ExKJ8FvNX4M9AG/At4I+SNi+U93nge8BawDjgEmAOsBHwFeALhbyfyMu3BbAu8DlgboO6HgUcAWxC2rmcm9NH5PkCIGlnYG1gbKMFV7I78F5gQo16bS+pUs5lwAa5DsMlbZnzDgeOioi1gIGk9Vc9n1VJ6+xXpO/oRuBThfG7ABcBxwLrA1cAf646QDgY2BPYEtgZ+FyD7aHaKNI6/g/g0by+mrEXcG007mvqbtJ2uj5wPWl7WKkw/iDSOuoD3Az8PC9zw3VS1ETedwG/BDYFKtvieQAR8Q3SdvilvH6+0cR3ukJzcOnZjgCGRcQLETELOAM4sl7miLgvIsZFxKJ8NnIp8J9Nzut2FgeSj5J2jn+vSru9MAzws4h4MyLGkH6khxbKuyYi7o2ItwCRdgLfjYhXI2ICUGz2e5MUBLZKixGTI+L5BnX9TUQ8GhEvA8OAw3P6taRAsGn+fCTwu4hYVKecXpLmkYLeRcDXIuLOwvgzI2JeRLxK2jk+FBFX5vU7DvgL8JmcdyGwjaS1IuLFiHiwxvx2A16PiIvyersSmFgY/2Xggoi4P8/jEmAVUoCrOC8iZkXEbGA0KZC1KyIWRsSIiHg5Il4jHbgMVnPXTNYnndk2Kn9kRMyNiDeBs/I07ylkuSUixubv4opCvdtbJ0UN8+b1cn3exuYDP6bx9t/ed7pCc3DpoSSJdIT5VCH5KdLRer1pts7NArMkvQR8n3RE1ow7gB3z0dwOpKO8icCWOe1DLL7e8m7g6aoj2eq6PVMY/g9SgHmmKn/FjaSjx18BsyRdJGnNBnWtLmd1SetExL+BPwFH5KPmQ0k7snoWRUSfiFg3IraOiF82mM9mwG65SWleDkqfIV13ADgwf346N7MMqjG/dwPTq9KK62Ez4DtV82hjyfX6XGH4FaDRenpbbhY7R+nGhZdIZy4iBYH2vMji5axX/mmSHpM0n3TWuSpLbnv16t3eOilqmFfSWpKGS3o6L+NNNN7+2/tOV2gOLj1U3nE/x5IXTDcFnq1kqTHZr4EHgC3yRdfTSTuQZub3MDCP1GT1aES8ls86xue0XnkYYEauS1GxbtX1ey5/7leVvzLviIifRcT2pGbA7YCvNahudTmv5CNVWNw0NgSYVecMolnFZXgGuCkHo8przYg4OS/D3RHxCVKz303A72uUNxOovhW8uB6fAb5fNY/VI+JPHaxrLccCewMfA9YhnSVCc9vH34BP1xsp6ePAV0lnAn1ITVavNll2e+ukI3lPzeMrNx3sXVWH6nXU8Dtd0Tm49Gy/B4bli7MbAv8P+G0eNwvYsOoIfy1gfkS8LGkb4L/omDuBU1jyekEl7Z7c5EEe/y5JJ+cj4o+Tfsh/rFVobob5C/BDpZsUPkhq8gPStQZJg/K1hX8DbwD1mrIAjpH03rzsPwD+UBh3G+mo+ExgZHOL3ZQ/k5rcDlW6qWLlXO/3SlpD0mH5DO9NYEGd+t8BrKr0H5/ekg4nBdOKS4Cv5nUhSWtK+pSk1ZuoX63toWgt4DXSWcgapCbWZv0E2FjSZZL6AUjqJ+l/lW4KWIu03LNJ18BOJ525NKO9ddKRvGuRzormSdoAqP7/yyyWbKqr+502WfcezcGlZ/s+8DAwmXSh+R+kHzqku7ZGAU/lU/r1gK8DX5L0MnAhS+50m3E7sCEpoFT8Pae9fQtyDhafIF1cfhH4GXBovs5Tz5dJR/WzSM1fvymM6wNcTjpzmkpq6ji/QVlXkALvs8BbwNv/T8lnfFcA2wC/a1BGh0TEXGAf0hnATNLZ2xlA5aL1F3K955MuDB9do4zKtZvjSU1H+5OCbmX8P4CTSOtnHunutc/R/lkJ1N4eii4j7fyfAyax5HfcUL7+9SHSst4vaQEwJpf1VF6GO4AnSN/fC3lezZTdcJ10MO85pGawF/Pyja4q4jzgqHxn2E+a+E5XaGp8A4fZikfSUOCzEbFXV9fFrLvymYtZgaQ1SNeILunquph1Zw4uZpmkTwHPA1OAa7q4OmbdmpvFzMysdD5zMTOz0q1wHeptsMEG0b9//66uhplZt3L//fe/EBFtzeZf4YJL//79GT9+fPsZzczsbZLq9XxQk5vFzMysdA4uZmZWOgcXMzMrnYOLmZmVzsHFzMxK5+BiZmalc3AxM7PSObiYmVnpHFzMzKx0LfuHvqRVSQ8AWiXP55qIGCbpcuA/SQ9FAjgmIibkZ77/AtiP9DS4YyLigVzW0Sx+KtwZETEip+9IekjUaqQH+3wt3BNn0/qfekNXV6Ep087ev6urYGYd1MruX14H9siPzF0JuFPSjXnctyKiukvzfYEB+bUzcDGwc34i3jBgEOmJevdLGpWfAncxMBS4hxRchgA3YmZmXaplzWKRvJw/rpRfjc4qDgBG5unuAfpI2pj0GNGxETEnB5SxwJA8bu2IuDufrYwEDmzV8piZWfNaes1FUi9JE0gPYBobEffmUWdKmijpPEmr5LRNgGcKk0/PaY3Sp9dIr1WPoZLGSxo/e3ZTj+Y2M7Nl0NLgEhGLImIg0BcYLGlb4DRgK2AnYD3gv3N21SpiKdJr1eOSiBgUEYPa2pruMdrMzJZSp9wtFhHzgNuAIRExMzd9vQ78Bhics00H+hUm6wvMaCe9b410MzPrYi0LLpLaJPXJw6sBewGP5msl5LvDDgQeypOMAo5SsgswPyJmAmOAvSWtK2ldYG9gTB63QNIuuayjgOtbtTxmZta8Vt4ttjEwQlIvUhC7OiL+KukWSW2kZq0JwHE5/2jSbchTSLciHwsQEXMk/QgYl/OdHhFz8vBXWHwr8o34TjEzs+VCy4JLREwEtq+Rvked/AGcUGfccGB4jfTxwLbLVlMzMyub/6FvZmalc3AxM7PSObiYmVnpHFzMzKx0Di5mZlY6BxczMyudg4uZmZXOwcXMzErn4GJmZqVzcDEzs9I5uJiZWekcXMzMrHQOLmZmVjoHFzMzK52Di5mZlc7BxczMSufgYmZmpXNwMTOz0jm4mJlZ6RxczMysdL1bVbCkVYE7gFXyfK6JiGGSNgeuAtYDHgCOjIg3JK0CjAR2BF4EDo2Iabms04AvAouAkyJiTE4fAvwC6AVcGhFnt2p5zLpC/1Nv6OoqNGXa2ft3dRVsOdPKM5fXgT0iYjtgIDBE0i7A/wDnRcQAYC4paJDf50bElsB5OR+StgYOA7YBhgAXSeolqRdwIbAvsDVweM5rZmZdrGXBJZKX88eV8iuAPYBrcvoI4MA8fED+TB6/pyTl9Ksi4vWIeBKYAgzOrykRMTUi3iCdDR3QquUxM7PmtfSaSz7DmAA8D4wFngDmRcTCnGU6sEke3gR4BiCPnw+sX0yvmqZeeq16DJU0XtL42bNnl7FoZmbWQEuDS0QsioiBQF/Smcb7a2XL76ozrqPptepxSUQMiohBbW1t7VfczMyWSafcLRYR84DbgF2APpIqNxL0BWbk4elAP4A8fh1gTjG9app66WZm1sVaFlwktUnqk4dXA/YCHgFuBQ7O2Y4Grs/Do/Jn8vhbIiJy+mGSVsl3mg0A7gPGAQMkbS5pZdJF/1GtWh4zM2tey25FBjYGRuS7ut4FXB0Rf5X0MHCVpDOAB4HLcv7LgCskTSGdsRwGEBGTJV0NPAwsBE6IiEUAkk4ExpBuRR4eEZNbuDxmZtaklgWXiJgIbF8jfSrp+kt1+mvAIXXKOhM4s0b6aGD0MlfWzMxK5X/om5lZ6RxczMysdA4uZmZWOgcXMzMrnYOLmZmVzsHFzMxK5+BiZmalc3AxM7PSObiYmVnpHFzMzKx0Di5mZlY6BxczMyudg4uZmZXOwcXMzErXyue5mHW6/qfe0NVVaMq0s/fv6iqYtZTPXMzMrHQOLmZmVjoHFzMzK52vuXSA2/PNzJrjMxczMytdy4KLpH6SbpX0iKTJkr6W038g6VlJE/Jrv8I0p0maIukxSfsU0ofktCmSTi2kby7pXkmPS/qDpJVbtTxmZta8Vp65LAS+ERHvB3YBTpC0dR53XkQMzK/RAHncYcA2wBDgIkm9JPUCLgT2BbYGDi+U8z+5rAHAXOCLLVweMzNrUsuCS0TMjIgH8vAC4BFgkwaTHABcFRGvR8STwBRgcH5NiYipEfEGcBVwgCQBewDX5OlHAAe2ZmnMzKwjOuWai6T+wPbAvTnpREkTJQ2XtG5O2wR4pjDZ9JxWL319YF5ELKxKrzX/oZLGSxo/e/bsEpbIzMwaaXlwkbQmcC1wckS8BFwMbAEMBGYC51ay1pg8liL9nYkRl0TEoIgY1NbW1sElMDOzjmrprciSViIFlisj4k8AETGrMP7XwF/zx+lAv8LkfYEZebhW+gtAH0m989lLMb+ZmXWhVt4tJuAy4JGI+FkhfeNCtoOAh/LwKOAwSatI2hwYANwHjAMG5DvDViZd9B8VEQHcChycpz8auL5Vy2NmZs1r5ZnLrsCRwCRJE3Lad0h3ew0kNWFNA74MEBGTJV0NPEy60+yEiFgEIOlEYAzQCxgeEZNzef8NXCXpDOBBUjAzM7Mu1rLgEhF3Uvu6yOgG05wJnFkjfXSt6SJiKuluMjMzW474H/pmZlY6BxczMyudg4uZmZWuQ8FFyRqtqoyZmfUM7QYXSSMlrS1pdWAy8KSkU1pfNTMz666aOXP5QP5n/YHATaQ/Kx7TykqZmVn31kxwWVlSb1LHkn/OnUe+1dpqmZlZd9ZMcLkUeBpYF7hd0qbAyy2tlZmZdWvtBpeIOC8i3h0Re+cuV54hdXVvZmZWU91/6Es6qZ1pzy+5LmZm1kM06v6l0jf9AFIXK3/Jnz8B3N7KSpmZWfdWN7hExPcAJI0BBuY7xpD0PeAPnVM9MzPrjpq5oL8Z8Frh8+vA5q2pjpmZ9QTN9Ir8O+BeSdeSusn/NPDbltbKzMy6tXaDS0ScLulGYLecdFxEjGtttczMrDtrGFwk9QIeiIjtSE+ENDMza1fDay75SZAPS9qkk+pjZmY9QDPXXDYAHpF0N/DvSmJEfLpltTIzs26tmeBydstrYWZmPUozF/RvlrQBMCgnjY+IF1pbLTMz686aeZ7LZ4AHgCOBo4Dxkg5qYrp+km6V9IikyZK+ltPXkzRW0uP5fd2cLknnS5oiaaKkHQplHZ3zPy7p6EL6jpIm5WnOl6SOrwIzMytbM3+i/D6wU0QcERGfA3YGftDEdAuBb0TE+4FdgBMkbQ2cCtwcEQOAm/NngH1JXc0MAIYCF0MKRsCwPN/BwLBKQMp5hhamG9JEvczMrMWaCS7viohZhc+zm5kuImZGxAN5eAHwCLAJ6bkwI3K2EaSHkJHTR0ZyD9BH0sbAPsDYiJgTEXOBscCQPG7tiLg799Y8slCWmZl1oWYu6N8kaTTpn/oAhwFjOjITSf2B7YF7gY0iYiakACRpw5xtE1J3/hXTc1qj9Ok10s3MrIs1E1y+CXwW2BUQ6WzjmmZnIGlN4Frg5Ih4qcFlkVojYinSa9VhKKn5jE033bS9KpuZ2TKq27wl6URJ2wOKiD9ExEkR8dWI+GNuhmqXpJVIgeXKiPhTTp6Vm7TI78/n9OlAv8LkfYEZ7aT3rZH+DhFxSUQMiohBbW1ttbKYmVmJGl072RL4FfCCpL9JOl3SPpLWbqbgfOfWZcAjEfGzwqhRQOWOr6OB6wvpR+W7xnYB5ufmszHA3pLWzRfy9wbG5HELJO2S53VUoSwzM+tCjZ7ncjKApFVJd2l9GDgeGCHp+Yj4YDtl70q6fXmSpAk57TukP2VeLemLwNPAIXncaGA/YArwCnBsrsccST9icd9mp0fEnDz8FeByYDXgxvwyM7Mu1sw1l97AysAq+TUDmNzeRBFxJ7WviwDsWSN/ACfUKWs4MLxG+nhg2/bqYmZmnatucJF0EfBB4FXgPuAe4EL/O9/MzNrT6JrLe4FVgaeAJ4ApDixmZtaMRtdc9pL0LtLZy4eB/yfp/aS7u+6KiB91Uh3NzKybaXjNJSLeAiZIeg6YlV8HkC7WO7iYmVlNja65HE86Y9mV1Hx2F3A3cCgwod50ZmZmjc5ctgL+CpwWEc80yGdmZraERtdcTurMipiZWc/RTK/IZmZmHeLgYmZmpXNwMTOz0jW6W2wutbuwF6m3lvVaViszM+vWGt0ttkGn1cLMzHqURneLLSp+zs+yX7WQVPPZKWZmZu1ec5G0v6R/kR7OdW9+v6XVFTMzs+6rmQv6Z5L+pf9YRPQD9gFua2WlzMyse2smuCyMiNnAuyQpIsYCO7S4XmZm1o0187Cw+ZLWAO4ERkp6HnirtdUyM7PurJkzlwOB14CTSc1hzwKfaGGdzMysm2smuJwWEYsi4s2IuCwifgac0uqKmZlZ99VMcBlSI23/sitiZmY9R93gIunLkh4E3ifpgcLrceDh9gqWNFzS85IeKqT9QNKzkibk136FcadJmiLpMUn7FNKH5LQpkk4tpG8u6V5Jj0v6g6SVl2YFmJlZ+RqduVwNHAKMzu+V164RcXgTZV9O7bOe8yJiYH6NBpC0NXAYsE2e5iJJvST1Ai4E9gW2Bg7PeQH+J5c1AJgLfLGJOpmZWSeoG1wiYm5ETImIQ4DVgI/nV1szBUfEHcCcJutxAHBVRLweEU8CU4DB+TUlIqZGxBvAVcABkgTsAVyTpx9BuvHAzMyWA838Q/8E0lnMpvl1dX4E8tI6UdLE3Gy2bk7bBCg+7XJ6TquXvj4wLyIWVqWbmdlyoJkL+l8GBkfEdyLiO8DOwHFLOb+LgS2AgcBM4Nycrhp5YynSa5I0VNJ4SeNnz57dsRqbmVmHNRNcBLxZ+PwmtXfu7YqIWfm25reAX5OavSCdefQrZO1L6hizXvoLQB9JvavS6833kogYFBGD2tqaatUzM7Nl0OhuscqO+wrgHknflfRd4C7SNY4Ok7Rx4eNBQOVOslHAYZJWkbQ5MAC4DxgHDMh3hq1Muug/KiICuBU4OE9/NHD90tTJzMzK16j7l/uAHSLiJ5JuBT5KOmM5LiLGtVewpN8DuwMbSJoODAN2lzSQ1IQ1jdTkRkRMlnQ16RbnhcAJlS7/JZ0IjAF6AcMjYnKexX8DV0k6A3gQuKwjC25mZq3TKLi83fSVg0m7AaWozu3KdQNARJxJ6oG5On006Xbo6vSpLG5WMzOz5Uij4NImqW43L7kbGDMzs3doFFx6AWuylBfvzcxsxdUouMyMiNM7rSZmZtZjNLoV2WcsZma2VBoFlz07rRZmZtajNOpbrNl+wczMzJbQzD/0zczMOsTBxczMSufgYmZmpXNwMTOz0jm4mJlZ6RxczMysdA4uZmZWOgcXMzMrnYOLmZmVzsHFzMxK5+BiZmalc3AxM7PSObiYmVnpHFzMzKx0Di5mZla6lgUXScMlPS/poULaepLGSno8v6+b0yXpfElTJE2UtENhmqNz/sclHV1I31HSpDzN+ZL85Ewzs+VEK89cLgeGVKWdCtwcEQOAm/NngH2BAfk1FLgYUjAChgE7A4OBYZWAlPMMLUxXPS8zM+siLQsuEXEHUP00ywOAEXl4BHBgIX1kJPcAfSRtDOwDjI2IORExFxgLDMnj1o6IuyMigJGFsszMrIv17uT5bRQRMwEiYqakDXP6JsAzhXzTc1qj9Ok10muSNJR0lsOmm266jItgZsui/6k3dHUVmjLt7P27ugrd2vJyQb/W9ZJYivSaIuKSiBgUEYPa2tqWsopmZtaszg4us3KTFvn9+Zw+HehXyNcXmNFOet8a6WZmthzo7OAyCqjc8XU0cH0h/ah819guwPzcfDYG2FvSuvlC/t7AmDxugaRd8l1iRxXKMjOzLtayay6Sfg/sDmwgaTrprq+zgaslfRF4GjgkZx8N7AdMAV4BjgWIiDmSfgSMy/lOj4jKTQJfId2RthpwY36ZmdlyoGXBJSIOrzNqzxp5AzihTjnDgeE10scD2y5LHc3MrDWWlwv6ZmbWgzi4mJlZ6RxczMysdA4uZmZWOgcXMzMrnYOLmZmVzsHFzMxK5+BiZmalc3AxM7PSObiYmVnpHFzMzKx0Di5mZlY6BxczMyudg4uZmZXOwcXMzErn4GJmZqVzcDEzs9I5uJiZWekcXMzMrHQOLmZmVrouCS6SpkmaJGmCpPE5bT1JYyU9nt/XzemSdL6kKZImStqhUM7ROf/jko7uimUxM7N36sozl49FxMCIGJQ/nwrcHBEDgJvzZ4B9gQH5NRS4GFIwAoYBOwODgWGVgGRmZl1reWoWOwAYkYdHAAcW0kdGcg/QR9LGwD7A2IiYExFzgbHAkM6utJmZvVNXBZcAbpJ0v6ShOW2jiJgJkN83zOmbAM8Upp2e0+qlv4OkoZLGSxo/e/bsEhfDzMxq6d1F8901ImZI2hAYK+nRBnlVIy0apL8zMeIS4BKAQYMG1cxjZmbl6ZIzl4iYkd+fB64jXTOZlZu7yO/P5+zTgX6FyfsCMxqkm5lZF+v04CJpDUlrVYaBvYGHgFFA5Y6vo4Hr8/Ao4Kh819guwPzcbDYG2FvSuvlC/t45zczMulhXNIttBFwnqTL/30XE/0kaB1wt6YvA08AhOf9oYD9gCvAKcCxARMyR9CNgXM53ekTM6bzFMDOzejo9uETEVGC7GukvAnvWSA/ghDplDQeGl11HMzNbNsvTrchmZtZDOLiYmVnpHFzMzKx0Di5mZlY6BxczMyudg4uZmZXOwcXMzErn4GJmZqVzcDEzs9I5uJiZWekcXMzMrHQOLmZmVjoHFzMzK52Di5mZlc7BxczMSufgYmZmpeuKJ1GamfUY/U+9oaur0JRpZ+/fqfPzmYuZmZXOwcXMzErn4GJmZqXr9sFF0hBJj0maIunUrq6PmZl18+AiqRdwIbAvsDVwuKStu7ZWZmbWrYMLMBiYEhFTI+IN4CrggC6uk5nZCk8R0dV1WGqSDgaGRMSX8ucjgZ0j4sSqfEOBofnj+4DHOrWijW0AvNDVlShRT1se6HnL1NOWB3reMi2Py7NZRLQ1m7m7/89FNdLeES0j4hLgktZXp+MkjY+IQV1dj7L0tOWBnrdMPW15oOctU09Ynu7eLDYd6Ff43BeY0UV1MTOzrLsHl3HAAEmbS1oZOAwY1cV1MjNb4XXrZrGIWCjpRGAM0AsYHhGTu7haHbVcNtctg562PNDzlqmnLQ/0vGXq9svTrS/om5nZ8qm7N4uZmdlyyMHFzMxKt8IGF0kvV30+RtIFefg4SUfl4cvz/2mQdJukltweKOmbkh6V9JCkfxbm//Y8JY2W1KfO9IskTZD0rKTJkk6RVPr3W28dFNdfIe3YXKcJkt6QNCkPn93OPCrL8k9JD0j6cNnL0V7d6+Q7UNLE/D1NknRgYVyHtw1JIemKwufekmZL+mv+/KlKl0Z53lsX8h4j6d11yr1c0pOFdbhnR+rVwWWofFcPSfpLZfuU9G5J17Qz7TRJG5RYl7d/qzXSn5W0Sv68gaTn8/r/aKWeknavrPtlqEPL9hGFedTcXhvtH5ZyPv0lPbS006+wwaWRiPhlRIxc1nJy9zTN5DsO+DgwOCK2BXajxn94ImK/iJhXp5hXI2Ig8DhwMrAfMGypKl6SiPhNRAzM9ZoBfCx/XqIPOEnVN5a8mvNtB5wG/LjZeSppRVDdDjgHOCAitgI+BZwj6YPLUOy/gW0lrZY/fxx4tjIyIkZFRCUQH0jq4qjiGKBmcMm+ldf7ycAvl6GO7al8V9sCc4ATACJiRkS8Y0dflhrbTHsWAV8ofF4DuBPYs6P1bPZ33dna2T90OgeXGiT9QNI328lzsaTx+Szhh4X0aZK+L+lO4FRJDxTGDZB0f43ivgMcHxEvAUTE/IgYUWOe0/JRV39Jj0j6dZ7/TXn8wcAg4ALSjudESXvmI8s5+TVB0sdyWX+WNFfSgvz5J+0tX1V9jpX0L0m3A7shJLJ+AAALzklEQVQ2Wl81pj1D0q8kjQV+I+lLkn5eGP9/kj4C9AE+LOmnue4v5jOHlyU9J2m/vD5mSpoKzAcel3Rvne9nJ0l35SP6+yStVVWv/SXdXeOI+pvAWRHxJEB+/zHwrUKez+eyH5I0OJc3OKc9mN/fl9OPAVYF1gSmKd31eDrQBuwqab18hHqLpEeAzwMjcr3/m/Q9X5nXyWrUdzewSWH59sx1mSRpeOFofpqks/Kyj5e0g6Qxkp7IBz+VI/vbJF2Tv4MrC+XuCHwI+Fqebqe8bTwgqZekc5Q6mH1F0lcL9ftqzjNJ0lbtrTNJf5T0F+AmJRdIeljSDcCGDdbDz4GvKwWlNYBVgC8CR2rx0fkhwCBJ1+XlniNpjfxdLJQ0S9KC/D1/X+lsaK6kGZKmSjqpUk7etv4l6aO57v0l/T0v69tn43md3i7p6pz/bElH5OknSdqiwTItQQ32D5VtRNKWkv6mxa0CW+T1+NO83U6SdGiNso+R9Cel3+XjKuwr6oqIFfJFOpKZUHg9DVyQx/0A+GYevhw4OA/fBgzKw+vl9145/YP58zTg24X53AoMzMNnAV+tqsdawNwG9SzOcxqpW4j+wMJCuVcDr9XIP5d0JPxj4DfAyFyHp4GngN8CU4FT8vingH7tLN9tpB3bxrmcNmBl4B+V9VdnOaYBGxQ+nwHcB6yaP38J+Hnhu3kp12c+qdeFj5Nunb8xvzYCngHG5/XxFvA8sC5p5/EoMLBY/1zPqcBOeT5r5zKPIQXkg4C/A+vWqP8DwHZVadsBDxTWy6/z8G7AQ8V55OG9gGvz8DG5zruQ/ps1Py/P7rmOJ+c8lxa2w2vI20/xe65R18tZvM0eCPwuD6+a5/He/HkkcHLh+/lKHj4PmEjaNtuA53P67rmefUkHpncDrwArAXcB1wNDgENJ2+RDpO3/TOBa0nb4VRZvW9MKy3N8YVkbrbPphek/DYzN3/G7gXmV5a61PoDhwLHAccCCPO5+Uv+EAB/L38lxpO3qvjzd/5K2wc8Ce5D2F+uR9hN3AVcCnwNeBG4Hzs3l7Qf8LQ+vzuJtfQAwvrBO55F+T6uQfq8/zOO+Rv5NVC3PMdT4rdF4//D5PHwvcFBhe1gd+ExhPW5E+l1vnMt5qDDPqcA6ebq39xX1Xt36fy7LqNKMBLx9JNmRttLPKvVZ1pv0RWxN+kEC/KGQ71LgWEmnkH50g6vKETW6rGnCkxExIQ/fT+0OO3uRNpStSD8QkZotngLeQ/oBvUoKDPsBDwObkXZAjZYPYGfgtoiYDSDpD8B7O7gM10fEazXSXyX9aM/In+8A/pbr0kY6Eh9D+iGtnvO8CPw1Iubm+jwJ/IUUYCv1D2BmRIwDiHymKAnSjmUQsHclvUqt76k67fe53Dskra3U/r0W6YxjQM67UiH/ooi4R+nayULgupy+gPTDngCsJ+nvwLbAGzTf39RP89HlhqQABqlfvScj4l/58wjS9lA5Y6z8AXkSsGZELAAWSHpNi9vy74uI6QCSJpC2g0m57JdJ208v0nqHtP3/MM/nIlLT75xCPf+U3+8nBQtIO7B662xsYfrdgN9HxCJghqRb2lknZ+VlfJbUJAlpGzkhDwfpN3EO8CtgMvCR/FpECnKLJK0P7E86IOqdl3kS6eBm5apl6p+HVwIukDQwl1X8rYyLiJkAkp4Absrpk0jb5dKo3j/0VzpL3yQirgOo/PaUWggq63GWUkvETiz5ewe4OSLm52mK+4qa3Cy2FCRtTmom2TMiPgjcQIrmFf8uDF9LeiTAJ4D7I+LFYll5R/ZvSe/pYDVeLwwvouoaTS7vLeDN6nEFC3M5i0g/kkVA7yaW7+3qd7DO1YrraSFLbo+V+d2T39uAI4DVgJ/mA4NZLN7xvFGpT67/zsAvqurfKJBPJQWCegFyMu88+NiBFJArqssO4EfArZGuSXySJddjJf8oUvPfHwvplQO/vYATSWcFf6T291DLt4Atge+SggjU3w4qKtvUWyy5fb1VqE/1dvcGqTlpHCkYDo+IDwBH5TzXko6Gd6bG9l8or7INQuN1VtxmoAPbYERMIX1f/wlsIGkaqUPbdZSPMPK8Xmbx9awgrbfX88634lzSGcE5wK/zdJXfYa1l+jppe92OtB2tXCirel0Xv4elPQGo/p56U//7b2+7aFRmXQ4uS2dt0kY+X9JGpOBRUz46GANcTGp6quXHwIWS1gbIR71D6+RtzwJSs8UvSU09/Uk7xiOAI0k/rk1JQaeeZpbvXmB3SetLWom0g1kW04Dt849cwI45/X35/UXSEe0rwFuSPkY6ciraOx9hb0g6o7m5qv6PAu+WtBOApLW0+MLwU6Qj55GStqlRv3OA0yT1z9P2J10rO7eQ59A87iPA/HyUtw6LL9IfU2fZh5OaRh6pMW4lYCbp+ygexS4gBcO6IuIt4BfAuyTtQ1r+/pK2zFmOJDXjLKvHSM1ElwLfzO37A3IdXiOdDZxGDnKS1munvGbWGaQz2sOUrulsTHNH+Q+RdpLTI6I/8GHSb+EjpObUbUlNXOsDJ5Eu+t9B3pFK2p10VhakbXFlUtNZe9YhnTW/RVrvnX5TQD6Qna58l6OkVSStTlq+Q/N6bCOdEd63rPNbkZvFllpE/FPSg6Sd9lRSs1IjV5J2XDfVGX8x6cLuOElvkjb2c+vkrad3bqZYj/RcmxdJ7e235rI2I/0gnib9YOsFuqaWLyJmSvoBqd19JumaxLL8YG4n7VAmkc5OXsl1fAV4JTdHXEk6Ij+FtC4fpXCxmrQj+B2wBSmI/q5Y/4h4I1+s/N+8A3yVdGZQWabHJB0B/FHSJyPiicK4CUoX0v+Sg+mbpGtrlaYHgLmS7iIF58qdST8hNfGcAtRstomI6UoXimu5N78qzXufzBd5Lwd+KelV4EMR8WqdskPSGbmuYyQdm5evN+lsY5nvJMvr9WDgfNJO9HHSgU3Fd4H/IzXVnUU60m9063e76yy7jnQNZBLwL5oLlLuTmnuK2818UkDZDJgCfJl0ULYRaZu6FThe0kTS9vh50vWX40nNlO39/iE1CV4r6ZBcXvUZWEcdo8Kt8Cxu+mzPkcCvJJ1O2oYPIa3HDwH/JO0jvh0Rz1UOpJaWu3/pBEp3nq0TEd/r6rr0VJK+BGwbESd3dV1sSd7+V0w+c2kxSdeRjqT36Oq6mHU2b/8rLp+5mJlZ6XxB38zMSufgYmZmpXNwMTOz0jm4mDVJ0n9Iukqp36mHlXqh7WivBNVlvt3zrKRBks6vk6/dHoQlfWdZ6mJWJgcXsybkP3deR+ryZouI2Jr0J8qNyppHRIyPiJPaz1mXg4stNxxczJrzMeDNiHj7T4f5D5QPSrpZi3v2PQDePiOp1zPtjkq90t7N4n6tlnieSO754CalnoF/RaGLDqXerO/P5Q7NaWcDqyn1knxlTvu8Uu+6E5R6oF4uu4q3nsnBxaw525I6AKz2GqmX2R1IAejcQj9VA4ALI2IbUvcun8npvwFOiogPNZjfMODOiNie1PfYpoVxX4iIHUl9VJ0kaf1Iz8ipPFvlCEnvJ3VHs2vuh20RqQsgs07hP1GaLRsBZ0najdTR4CYsbiqr1TPtOkCfiKh0VXIFtftu243cS3BE3CBpbmHcSZIOysP9SEGsukPIPUn9s43LsW41Uq+9Zp3CwcWsOZOp3UHhEaQem3eMiDdzT7uVXnyre5FdjY49YuEd+XLHiXuR+hN7RdJt1O4pWcCIiDityXmZlcrNYmbNuQVYRdJ/VRJy78qbkR6m9WadnpqXEOkxtPNzz8lQv6nqjso4SfuSHoIGqWPIuTmwbMWSHRa+mTvVBLgZOFjShrmM9SQ1rJtZmRxczJoQqZ+kg4CP51uRJ5OeRDia9Gjc8aRg8GgTxR1LesTC3aSemWv5IbCb0mOy9yb1Zg2pd+HeuYfeH7H4eTcAlwATJV0ZEQ+TeiO+KecdS+pV2axTuG8xMzMrnc9czMysdA4uZmZWOgcXMzMrnYOLmZmVzsHFzMxK5+BiZmalc3AxM7PS/X/shpuR/49GqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Below are the lines to create our column graph visualization.\n",
    "\n",
    "plt.bar(df['Candidate'], height=df['Total_Words'])\n",
    "plt.xlabel('Candidate')\n",
    "plt.ylabel('Total Words')\n",
    "plt.title('Total Words by Presidential Candidate')\n",
    "plt.rcParams[\"figure.figsize\"] = [10,10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Five: Top Ten Most Frequent Words (aka Term Frequency)\n",
    "Next we will list the top words used by one of our candidates to analyze the party platform closer. The frequency of words can tell us what topics are being discussed, and the primary issues the candidate chooses to focus for their campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(567, 'the'),\n",
       " (475, 'and'),\n",
       " (454, 'of'),\n",
       " (349, 'to'),\n",
       " (200, 'we'),\n",
       " (187, 'in'),\n",
       " (139, 'will'),\n",
       " (138, 'a'),\n",
       " (115, 'for'),\n",
       " (111, 'our')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function creates a dictionary that counts tokens and assigns a numer as the value.\n",
    "# We will use Richard Nixon's 1968 platform as our corpus.\n",
    "\n",
    "def wordfreq(file):\n",
    "    counter = dict()\n",
    "    for word in file:\n",
    "        if word not in counter:\n",
    "            counter[word] = 1\n",
    "        else:\n",
    "            counter[word] += 1\n",
    "    return counter\n",
    "\n",
    "nixon_word_freq = wordfreq(nixon_clean)\n",
    "\n",
    "\n",
    "\n",
    "# In its current form, the variable will return an unordered list.\n",
    "# This next function sorts the dictionary by converting it into a list of tuple and sorting it\n",
    "# into the top 10 words and the count.\n",
    "\n",
    "def sorter(file):\n",
    "    newlist = []\n",
    "    for key, value in list(file.items()):\n",
    "        newlist.append((value,key))\n",
    "    newlist.sort(reverse=True)\n",
    "    return newlist\n",
    "nixon_freq_sorted = sorter(nixon_word_freq)\n",
    "nixon_freq_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the list right now is not very useful. It is filled with what we call stopword, the parts of speech like 'the,' 'and,' and 'or,' that do not add very much to the meaning of our speech. We will reconsider this problem in the second exercise where we will use natural language tools and remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Using counting tools to answer a simple research question.\n",
    "\n",
    "In this section, we will use the simple counting code that we have used to answer a simple research question and hypothesis.\n",
    "\n",
    "Question: Do Republicans and Democrats appeal to different audiences using their party platform language?\n",
    "\n",
    "Hypothesis: Contemporary republicans (in this case Donald Trump, Mitt Romney, and Richard Nixon) will use variations of the word 'America' to emphasis patriotism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate</th>\n",
       "      <th>America_Total</th>\n",
       "      <th>America_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>51</td>\n",
       "      <td>0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>42</td>\n",
       "      <td>0.001179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>60</td>\n",
       "      <td>0.002259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mitt Romney</td>\n",
       "      <td>44</td>\n",
       "      <td>0.001441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard Nixon</td>\n",
       "      <td>12</td>\n",
       "      <td>0.001205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Candidate  America_Total  America_Ratio\n",
       "0  Hillary Clinton             51       0.001964\n",
       "1     Donald Trump             42       0.001179\n",
       "2     Barack Obama             60       0.002259\n",
       "3      Mitt Romney             44       0.001441\n",
       "4    Richard Nixon             12       0.001205\n",
       "5  Abraham Lincoln              0       0.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code is similar to the counting code above for gerunds and tokens, but counts our target words.\n",
    "\n",
    "def amercount(file):\n",
    "    count = 0\n",
    "    for word in file:\n",
    "        if word[0:8] == 'america':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "clinton_amer = amercount(clinton_clean)\n",
    "clinton_amer_ratio = clinton_amer/clinton_words\n",
    "\n",
    "trump_amer = amercount(trump_clean)\n",
    "trump_amer_ratio = trump_amer/trump_words\n",
    "\n",
    "obama_amer = amercount(obama_clean)\n",
    "obama_amer_ratio = obama_amer/obama_words\n",
    "\n",
    "romney_amer = amercount(romney_clean)\n",
    "romney_amer_ratio = romney_amer/romney_words\n",
    "\n",
    "nixon_amer = amercount(nixon_clean)\n",
    "nixon_amer_ratio = nixon_amer/nixon_words\n",
    "\n",
    "lincoln_amer = amercount(lincoln_clean)\n",
    "lincoln_amer_ratio = lincoln_amer/lincoln_words\n",
    "\n",
    "# Similar to above, we assemble the data into a pandas dataframe.\n",
    "\n",
    "data = {'Candidate': ['Hillary Clinton', 'Donald Trump', 'Barack Obama', 'Mitt Romney', 'Richard Nixon', 'Abraham Lincoln'], \n",
    "        'America_Total': [clinton_amer, trump_amer, obama_amer, romney_amer, nixon_amer, lincoln_amer],\n",
    "        'America_Ratio': [clinton_amer_ratio, trump_amer_ratio, obama_amer_ratio, romney_amer_ratio, nixon_amer_ratio, lincoln_amer_ratio]}\n",
    "\n",
    "df2 = pandas.DataFrame(data, columns = ['Candidate', \n",
    "                                       'America_Total', \n",
    "                                       'America_Ratio'])\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic results show us that our simple hypothesis was wrong. In fact, although the <i>n</i> is small, it appears that the two Democratic candidates (Barack Obama and Hilary Clinton) used variations of the word 'America' more frequently then the Republican candidiates, based upon word frequency ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 2\n",
    "## Natural Language Processing\n",
    "Computational text analysis is good for analyzing large corpora, thousands, if not millions, of words long. The basic counting tools we used in exercise one forms the core of our digital research methodologies, but we want to do more. In this exercise, we will be applying natural language processing (NLP) tools to texts to achieve a broad, zoomed-out view of our data. NLP refers to many methods that computers use language text. Humans speak in <i>natural language</i>, but computers \"speak\" in a numerical <i>formal language</i> - NLP tools help us bridge the gap when using computational text analysis.\n",
    "\n",
    "This exercise has been adaption from a NLP practicum designed by Professor Laura Nelson of the Sociology Department at Northeastern University.\n",
    "\n",
    "\n",
    "## Data\n",
    "For this exercise, we are comparing the same political party platform documents from \"The American Presidency Project\". Our corpus consists of every Republican and Democrat party platform since the 2000 election. However, instead of analyzing each document individually, we have combined them into two documents: one consisting of all the Democrats and the other is all of the Republicans.\n",
    "\n",
    "1. Democrats (Gore, Kerry, Obama(2008), Obama(2012), Clinton)\n",
    "2. Republicans (Bush(2000), Bush(2004), McCain, Romney, Trump)\n",
    "\n",
    "\n",
    "\n",
    "## Libraries Used:\n",
    "1. String\n",
    "2. Nltk\n",
    "\n",
    "\n",
    "\n",
    "## <u>New</u> Key Terminology\n",
    "<b> Token:</b> <br>\n",
    "<b> Pre-Processing:</b> <br>\n",
    "<b> POS Tagging:</b> <br>\n",
    "<b> NLTK (Natural Lanugage Tool Kit):</b> <br>\n",
    "<b> Concordance:</b> <br>\n",
    "<b> Document Term Matrix (DTM):</b> <br>\n",
    "<b> TF-DIF Scores:</b> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first basic step is to reset all of our variables from exercise one so we have a clean slate -\n",
    "# though we will need to repeat some functions.\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines load in our required Python libraries.\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: Reading and Tokenizing\n",
    "In the first step, we will read in our collection of political platforms similar to exercise one, then we will tokenize it using the 'nltk' package instead of the method used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK has a number of built in models, but to save time, we only want a few of them so we will designate a variable to\n",
    "# download only specific models we will use in this exercise. \n",
    "# For a list of models and built-in NLTK corpora, see: http://www.nltk.org/nltk_data/\n",
    "\n",
    "# This variable is our model data variable, each string is a different model that can be found in the link above.\n",
    "nltkdata = [\"punkt\", \"words\", \"stopwords\", \"averaged_perceptron_tagger\", \"maxent_ne_chunker\", 'wordnet']\n",
    "\n",
    "# This line will download the specific models we want to use - alternatively, we could download the entire nltk package.\n",
    "nltk.download(nltkdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the same readfile function seen in exercise one.\n",
    "\n",
    "def read_file(name):\n",
    "    with open(name, 'r', encoding = 'utf-8') as my_file:\n",
    "        return my_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will import the stopwords library that we downloaded for nltk above.\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These variables use our read function above to read in our texts.\n",
    "\n",
    "dems = read_file('data/Democrats.txt')\n",
    "reps = read_file('data/Republicans.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will use nltk to tokenize our text.\n",
    "\n",
    "dems_tokens = nltk.word_tokenize(dems)\n",
    "reps_tokens = nltk.word_tokenize(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will need to pre-process the text similar to what we did in exercise one.\n",
    "# This includes: making all the text lowercase, removing punctuation, removing stopwords.\n",
    "# As we saw in exercise one, step five, the most frequently used words are stopwords, so it is useful to remove them.\n",
    "\n",
    "dems_clean = [word.lower() for word in dems_tokens if \n",
    "                 (word.lower() not in stopwords.words('english')) &\n",
    "                (word not in string.punctuation)]\n",
    "\n",
    "reps_clean = [word.lower() for word in reps_tokens if \n",
    "                 (word.lower() not in stopwords.words('english')) &\n",
    "                (word not in string.punctuation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preamble',\n",
       " '2016',\n",
       " 'democrats',\n",
       " 'meet',\n",
       " 'philadelphia',\n",
       " 'basic',\n",
       " 'belief',\n",
       " 'animated',\n",
       " 'continental',\n",
       " 'congress',\n",
       " 'gathered',\n",
       " '240',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'many']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out our tokens\n",
    "\n",
    "dems_clean[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 749),\n",
       " ('america', 484),\n",
       " ('must', 469),\n",
       " ('support', 442),\n",
       " ('american', 441),\n",
       " ('democrats', 426),\n",
       " ('health', 404),\n",
       " ('americans', 398),\n",
       " ('new', 390),\n",
       " ('people', 380),\n",
       " ('work', 372),\n",
       " ('believe', 325),\n",
       " ('president', 307),\n",
       " ('security', 298),\n",
       " ('also', 292),\n",
       " ('make', 288),\n",
       " ('care', 273),\n",
       " ('world', 272),\n",
       " ('need', 251),\n",
       " ('jobs', 243)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now use nltk to count the most frequent tokens, similar to exercise one, step five.\n",
    "# The function is .FreqDist()\n",
    "\n",
    "dems_freq = nltk.FreqDist(dems_clean)\n",
    "dems_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, tokenization is not perfect because the most commonly used token is 's - used in contractions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Two: Part-of-Speech (POS) Tagging\n",
    "Stopwords and tokens present challenges to computational text analysis. One alternative is to use the built-in POS tagging system in nltk to assign the part-of-speech to each token (e.g., noun, adjective, adverb, etc.). We can then see how different parts of speech are used in the party platforms. The POS tagger comes from the University of Pennsylvania treebank corpus, and a complete list can be found here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('preamble', 'JJ'),\n",
       " ('2016', 'CD'),\n",
       " ('democrats', 'NNPS'),\n",
       " ('meet', 'VBP'),\n",
       " ('philadelphia', 'JJ'),\n",
       " ('basic', 'JJ'),\n",
       " ('belief', 'NN'),\n",
       " ('animated', 'VBD'),\n",
       " ('continental', 'JJ'),\n",
       " ('congress', 'NN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function is the built-in POS tagger.\n",
    "\n",
    "dems_tagged = nltk.pos_tag(dems_clean)\n",
    "reps_tagged = nltk.pos_tag(reps_clean)\n",
    "\n",
    "# Let's see what this looks like.\n",
    "\n",
    "dems_tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 22074),\n",
       " ('JJ', 14089),\n",
       " ('NNS', 12038),\n",
       " ('VBP', 5119),\n",
       " ('VBG', 3526),\n",
       " ('RB', 2657),\n",
       " ('VB', 2050),\n",
       " ('VBD', 1869),\n",
       " ('VBZ', 1096),\n",
       " ('VBN', 995)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's combine POS and .FreqDist() to see the most used POS.\n",
    "\n",
    "dems_tag_freq = nltk.FreqDist([tag for word, tag in dems_tagged])\n",
    "reps_tag_freq = nltk.FreqDist([tag for word, tag in reps_tagged])\n",
    "\n",
    "# Again, let's see what this looks like.\n",
    "\n",
    "dems_tag_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN's are Nouns, JJ's are adjectives, NNS' are plural nouns, and we can see the rest at the UPenn link above.\n",
    "\n",
    "We can now combine our POS and counting methods to see the most used nouns, adjectives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will count the top nouns\n",
    "\n",
    "dems_noun = [word for word,pos in dems_tagged if pos == 'NN' or pos=='NNS' or pos=='NNP' or pos=='NNPS']\n",
    "reps_noun = [word for word,pos in reps_tagged if pos == 'NN' or pos=='NNS' or pos=='NNP' or pos=='NNPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('health', 404),\n",
       " ('democrats', 391),\n",
       " ('people', 380),\n",
       " ('americans', 312),\n",
       " ('president', 307),\n",
       " ('support', 304),\n",
       " ('security', 298),\n",
       " ('world', 272),\n",
       " ('care', 259),\n",
       " ('america', 257),\n",
       " ('jobs', 243),\n",
       " ('families', 243),\n",
       " ('work', 241),\n",
       " ('government', 238),\n",
       " ('rights', 234)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we will see what the most commonly nouns are for the democratic platform.\n",
    "\n",
    "dems_noun_freq = nltk.FreqDist(dems_noun).most_common\n",
    "dems_noun_freq(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('president', 595),\n",
       " ('government', 570),\n",
       " ('states', 478),\n",
       " ('people', 459),\n",
       " ('health', 425),\n",
       " ('support', 381),\n",
       " ('care', 341),\n",
       " ('world', 322),\n",
       " ('administration', 312),\n",
       " ('america', 311),\n",
       " ('security', 292),\n",
       " ('nation', 291),\n",
       " ('tax', 291),\n",
       " ('law', 266),\n",
       " ('americans', 253)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do the same for the republicans.\n",
    "\n",
    "reps_noun_freq = nltk.FreqDist(reps_noun).most_common\n",
    "reps_noun_freq(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do the same for verbs and adjectives.\n",
    "\n",
    "# Verb function:\n",
    "\n",
    "dems_verb = [word for word,pos in dems_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "reps_verb = [word for word,pos in reps_tagged if pos == 'VB' or pos=='VBD' or pos=='VBG' or pos=='VBN' or pos=='VBP' or pos=='VBZ']\n",
    "\n",
    "# Adjective function:\n",
    "\n",
    "dems_adj = [word for word,pos in dems_tagged if pos == 'JJ' or pos=='JJR' or pos=='JJS']\n",
    "reps_adj = [word for word,pos in reps_tagged if pos == 'JJ' or pos=='JJR' or pos=='JJS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('believe', 315), ('make', 273), ('continue', 197), ('ensure', 197), ('including', 184), ('need', 178), ('committed', 144), ('work', 129), ('support', 125), ('working', 122), ('create', 107), ('provide', 107), ('protect', 94), ('help', 94), ('take', 94)]\n",
      "[('american', 441), ('new', 390), ('democratic', 207), ('public', 204), ('national', 202), ('economic', 201), ('global', 169), ('federal', 144), ('united', 141), ('nuclear', 137), ('military', 133), ('international', 128), ('many', 124), ('strong', 120), ('good', 104)]\n"
     ]
    }
   ],
   "source": [
    "# Let's also see what they look like.\n",
    "\n",
    "dems_verb_freq = nltk.FreqDist(dems_verb).most_common\n",
    "dems_adj_freq = nltk.FreqDist(dems_adj).most_common\n",
    "print(dems_verb_freq(15))\n",
    "print(dems_adj_freq(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ensure', 194), ('make', 190), ('including', 174), ('republicans', 136), ('believe', 129), ('support', 123), ('made', 115), ('continue', 103), ('need', 102), ('provide', 98), ('protect', 97), ('america', 94), ('oppose', 90), ('—', 87), ('recognize', 84)]\n",
      "[('american', 561), ('federal', 485), ('republican', 410), ('new', 392), ('national', 330), ('united', 278), ('economic', 270), ('current', 233), ('public', 225), ('u.s.', 212), ('free', 207), ('military', 202), ('private', 194), ('international', 170), ('political', 160)]\n"
     ]
    }
   ],
   "source": [
    "# And the same for the republicans.\n",
    "\n",
    "reps_verb_freq = nltk.FreqDist(reps_verb).most_common\n",
    "reps_adj_freq = nltk.FreqDist(reps_adj).most_common\n",
    "print(reps_verb_freq(15))\n",
    "print(reps_adj_freq(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see how language is used differently betweeen the two party platforms. While many of the words are similar (American, Presidents, believe, etc.), there are also interesting differences. For example the Democratic usage of the words 'jobs,' 'families,' and 'work' reinforce the historical socially-focused democratic party as we know it, while the Republican usage of the words 'security,' 'tax,' and 'order' reinforce the law & order and national security foci of Republican party history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three: nGrams\n",
    "In this next step of analysis we will look at the most commonly used nGrams (bigrams and trigrams) of the political platforms. These functions are built into nltk. We will use the variables we have created with the stopwords removed, but it may also be informative to just use the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will extract bigrams from both the democratic and republican platforms and see what the most common are.\n",
    "\n",
    "dems_bigrams = ngrams(dems_clean, 2)\n",
    "dems_bigrams_freq = nltk.FreqDist(dems_bigrams).most_common\n",
    "\n",
    "reps_bigrams = ngrams(reps_clean, 2)\n",
    "reps_bigrams_freq = nltk.FreqDist(reps_bigrams).most_common\n",
    "\n",
    "# We can also do trigrams.\n",
    "\n",
    "dems_trigrams = ngrams(dems_clean, 3)\n",
    "dems_trigrams_freq = nltk.FreqDist(dems_trigrams).most_common\n",
    "\n",
    "reps_trigrams = ngrams(reps_clean, 3)\n",
    "reps_trigrams_freq = nltk.FreqDist(reps_trigrams).most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('health', 'care'), 158), (('president', 'obama'), 142), (('democratic', 'party'), 138), (('united', 'states'), 134), (('america', \"'s\"), 117), (('democrats', 'believe'), 103), (('middle', 'class'), 84), (('al', 'gore'), 84), (('nation', \"'s\"), 73), (('climate', 'change'), 60)]\n",
      "[(('president', 'obama', 'democratic'), 23), (('obama', 'democratic', 'party'), 23), (('president', 'obama', \"'s\"), 22), (('john', 'kerry', 'john'), 21), (('kerry', 'john', 'edwards'), 21), (('democratic', 'party', 'believe'), 19), (('president', 'obama', 'democrats'), 18), (('president', 'democratic', 'party'), 17), (('al', 'gore', 'democratic'), 16), (('gore', 'democratic', 'party'), 16)]\n",
      "[(('united', 'states'), 253), (('america', \"'s\"), 236), (('president', 'bush'), 218), (('health', 'care'), 195), (('nation', \"'s\"), 131), (('current', 'administration'), 117), (('federal', 'government'), 114), (('american', 'people'), 104), (('republican', 'congress'), 93), (('president', \"'s\"), 87)]\n",
      "[(('president', 'bush', \"'s\"), 58), (('current', 'administration', \"'s\"), 42), (('president', 'bush', 'republican'), 31), (('weapons', 'mass', 'destruction'), 30), (('bush', 'republican', 'congress'), 29), (('george', 'w.', 'bush'), 28), (('applaud', 'president', 'bush'), 28), (('supreme', 'court', \"'s\"), 22), (('september', '11', '2001'), 22), (('support', 'president', 'bush'), 19)]\n"
     ]
    }
   ],
   "source": [
    "print(dems_bigrams_freq(10))\n",
    "print(dems_trigrams_freq(10))\n",
    "print(reps_bigrams_freq(10))\n",
    "print(reps_trigrams_freq(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see some trends from the bigrams and trigram frequencies. Many of the ngrams we see are the names of the candidates, which is not entirely surprising, but it is informative seeing 'climate change' appearing frequently in the Democratic platforms while seeing \"weapons mass destruction\" (remember we have removed stopwords) appearing frequently in the Republican platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Four: Concordances and Similarity\n",
    "In this phase of analysis, we will use concordance analysis to see the words that surround specific words. This will show us the words that appear before and after it. We will not use the tokenized version of the text, not the version with stopwords removed - this will allow us to see the full context surrounding a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to turn our tokenized lists into nltk 'text-objects'.\n",
    "\n",
    "dems_nltk = nltk.Text(dems_tokens)\n",
    "reps_nltk = nltk.Text(reps_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 46 matches:\n",
      "temic racism to our deeply broken immigration system to discrimination against \n",
      "ustice system . Fixing our Broken Immigration System The United States was foun\n",
      "And that is why Democrats believe immigration is not just a problem to be solve\n",
      "e Democratic Party supports legal immigration , within reasonable limits , that\n",
      "t , we recognize that the current immigration system is broken . More than 11 m\n",
      "ithout proper documentation . The immigration bureaucracy is full of backlogs t\n",
      "e need to urgently fix our broken immigration system—which tears families apart\n",
      " would exclude or eliminate legal immigration avenues and denigrate immigrants \n",
      "ntinue to fight for comprehensive immigration reform , we will defend and imple\n",
      " take that last step . We believe immigration enforcement must be humane and co\n",
      "sel for unaccompanied children in immigration courts . We should consider all a\n",
      " that all Americans—regardless of immigration status—have access to quality hea\n",
      "nd finally enacting comprehensive immigration reform . And we will expand oppor\n",
      "vulnerable communities within our immigration system who are often seeking refu\n",
      "riate , and that neither fear nor immigration status should be barriers that im\n",
      "mic development and comprehensive immigration reform . And we will build on our\n",
      "engthening the American Community Immigration . Democrats are strongly committe\n",
      "mmitted to enacting comprehensive immigration reform that supports our economic\n",
      "my . Our prosperity depends on an immigration system that reflects our values a\n",
      "t Americans know that today , our immigration system is badly broken - separati\n",
      "ntry urgently needs comprehensive immigration reform that brings undocumented i\n",
      " to earn citizenship . We need an immigration reform that creates a system for \n",
      "terest , Republicans have blocked immigration reform in Congress and used the i\n",
      "mportant progress in implementing immigration policies that reward hard work an\n",
      " streamlined the process of legal immigration for immediate relatives of U.S. c\n"
     ]
    }
   ],
   "source": [
    "# Now we can see the concordances. First we will do the Democrats.\n",
    "\n",
    "dems_nltk.concordance(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 36 matches:\n",
      "ter identification ( ID ) laws to immigration , from healthcare programs to lan\n",
      "ders picking winners and losers . Immigration and the Rule of Law Our party is \n",
      "nomy is the American worker . Our immigration system must protect American work\n",
      "ur nation as a whole . America 's immigration policy must serve the national in\n",
      " and disgust , the mocking of our immigration laws by a president who made hims\n",
      "s of murdered innocents . Illegal immigration endangers everyone , exploits the\n",
      "ports of entry and to enforce our immigration laws . That is why we support bui\n",
      "y to take steps to reduce illegal immigration . We condemn the Obama Administra\n",
      " in nuclear proliferation . While immigration is addressed in more detail elsew\n",
      "ty issue , and that our nation 's immigration and refugee policies are placing \n",
      " secure our borders , enforce our immigration laws , and properly screen refuge\n",
      "al trade—by a policy of strategic immigration , granting more work visas to hol\n",
      "onal principle—in its handling of immigration cases , in federal personnel bene\n",
      "ers ranging from voter ID laws to immigration , from healthcare programs to lan\n",
      "risking . The Rule of Law : Legal Immigration The greatest asset of the America\n",
      "h the rest of the world . Illegal immigration undermines those benefits and aff\n",
      "ent Administration 's approach to immigration has undermined the rule of law at\n",
      "agreements in Section 287g of the Immigration and Nationality Act to make commu\n",
      ". State efforts to reduce illegal immigration must be encouraged , not attacked\n",
      "' opposition to it was so wrong . Immigration , National Security , and the Rul\n",
      "al Security , and the Rule of Law Immigration policy is a national security iss\n",
      "ns know America can have a strong immigration system without sacrificing the ru\n",
      "in reducing and reversing illegal immigration . Our commitment to the rule of l\n",
      "cans have is with a dysfunctional immigration bureaucracy defined by delay and \n",
      "iate transfer of their custody to Immigration and Customs Enforcement . Locking\n"
     ]
    }
   ],
   "source": [
    "# And we can do the same for the Republicans.\n",
    "\n",
    "reps_nltk.concordance(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education justice medicare intelligence that and it businesses now\n",
      "growth health americans security while families as so believe bridges\n",
      "investment\n"
     ]
    }
   ],
   "source": [
    "# We can also see the contextual similarity surrounding the word and see what words appear in a similar context.\n",
    "# We start with the Democratic platforms.\n",
    "\n",
    "dems_nltk.similar(\"immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political education tax federal legal health states country strength\n",
      "economy national schools jobs free justice learn work prosperity\n",
      "current taxation\n"
     ]
    }
   ],
   "source": [
    "# And the same for the Republican platforms.\n",
    "\n",
    "reps_nltk.similar(\"immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Five: Difference of Proportions\n",
    "Due to the nature of language, the Democratic and Republican platforms use very similar words. So the next steps should be to see how different the texts are. This is a more elaborate process of transforming text counts into proportions then subtracting one from the other.\n",
    "\n",
    "We will be using the CountVectorizer function that is part of the `scikit-learn` machine-learning library we imported at the beginning of the exercise. For more information, see here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x11513 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16883 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will first need to convert our text into a document term matrix (DTM - see the terminology above). \n",
    "# DTM is a method of vector representation - it will transform the text into numbers.\n",
    "\n",
    "# First we define our corpus.\n",
    "\n",
    "corpus = [dems, reps]\n",
    "\n",
    "# Next we define our vectorizer.\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Then we trasnform our text to a DTM.\n",
    "textdtm = vect.fit_transform(corpus)\n",
    "textdtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 559)\t1\n",
      "  (0, 4972)\t1\n",
      "  (0, 7676)\t1\n",
      "  (0, 6964)\t2\n",
      "  (0, 686)\t2\n",
      "  (0, 6272)\t2\n",
      "  (0, 6959)\t1\n",
      "  (0, 6441)\t1\n",
      "  (0, 8734)\t1\n",
      "  (0, 262)\t1\n",
      "  (0, 6661)\t1\n",
      "  (0, 5115)\t1\n",
      "  (0, 8807)\t1\n",
      "  (0, 2432)\t1\n",
      "  (0, 6921)\t1\n",
      "  (0, 7307)\t1\n",
      "  (0, 6161)\t1\n",
      "  (0, 6790)\t1\n",
      "  (0, 1383)\t1\n",
      "  (0, 7006)\t1\n",
      "  (0, 6489)\t1\n",
      "  (0, 10380)\t1\n",
      "  (0, 10666)\t1\n",
      "  (0, 11161)\t1\n",
      "  (0, 1017)\t1\n",
      "  :\t:\n",
      "  (1, 8398)\t7\n",
      "  (1, 4742)\t77\n",
      "  (1, 11270)\t96\n",
      "  (1, 6253)\t126\n",
      "  (1, 1981)\t32\n",
      "  (1, 7495)\t459\n",
      "  (1, 647)\t563\n",
      "  (1, 2937)\t27\n",
      "  (1, 11417)\t240\n",
      "  (1, 4884)\t48\n",
      "  (1, 10386)\t12\n",
      "  (1, 6033)\t164\n",
      "  (1, 7030)\t11\n",
      "  (1, 7881)\t595\n",
      "  (1, 536)\t51\n",
      "  (1, 11483)\t178\n",
      "  (1, 4586)\t1\n",
      "  (1, 2174)\t330\n",
      "  (1, 2283)\t3\n",
      "  (1, 1172)\t15\n",
      "  (1, 1120)\t31\n",
      "  (1, 6531)\t58\n",
      "  (1, 2814)\t48\n",
      "  (1, 106)\t2\n",
      "  (1, 7803)\t5\n"
     ]
    }
   ],
   "source": [
    "# We can also see what this looks like\n",
    "\n",
    "print(textdtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yezidis', 'yield', 'yielded', 'yielding', 'yields', 'york', 'yosemite', 'young', 'younger', 'youngest', 'youngsters', 'youth', 'youths', 'yucca', 'yugoslav', 'zealand', 'zenith', 'zero', 'zika', 'zimbabwe', 'zimbabwean', 'zip', 'zone', 'zones', 'zoning']\n"
     ]
    }
   ],
   "source": [
    "# These numbers look incomprehensible, but they indicate words in our texts.\n",
    "# We can use the .get_feature_names() function to see what they represent.\n",
    "\n",
    "print(vect.get_feature_names()[-25:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>104</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>111</th>\n",
       "      <th>115</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zero</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dems</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reps</th>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 11513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  10  100  1000  104  10th  11  111  115   ...    zealand  \\\n",
       "dems   1   54  13   13     1    0     0  18    0    0   ...          3   \n",
       "reps   0   54  15   28     0    1     1  32    1    1   ...          3   \n",
       "\n",
       "      zenith  zero  zika  zimbabwe  zimbabwean  zip  zone  zones  zoning  \n",
       "dems       0     4     2         3           0    3     4      5       0  \n",
       "reps       1     9     1         5           1    3     4      5       5  \n",
       "\n",
       "[2 rows x 11513 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the data form is compressed, it saves a lot of memory.\n",
    "# But we can also see what the data would look like in a pandas dataframe.\n",
    "\n",
    "dtm_df = pandas.DataFrame(vect.fit_transform(corpus).toarray(), columns=vect.get_feature_names(), index = ['dems', 'reps'])\n",
    "\n",
    "#view the dtm dataframe\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the function as applied numbers to our words based upon usage. and there are a lot of words (11,513 columns).\n",
    "\n",
    "Next, we will transform each cell of our matrix into a proportion rather than a raw usage count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dems    65472\n",
       "reps    91023\n",
       "Name: sum, dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to create a raw word count, summing the counts we have.\n",
    "\n",
    "dtm_df['sum'] = dtm_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dems    65472\n",
       "reps    91023\n",
       "Name: sum, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what our words counts are.\n",
    "\n",
    "dtm_df['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proportion is a simple calculation of dividing the colmuns by the sum.\n",
    "\n",
    "dtm_df_prop = dtm_df.loc[:,\"00\":\"zoning\"].div(dtm_df[\"sum\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>104</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>111</th>\n",
       "      <th>115</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zero</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dems</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reps</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 11513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00       000        10       100      1000       104      10th  \\\n",
       "dems  0.000015  0.000825  0.000199  0.000199  0.000015  0.000000  0.000000   \n",
       "reps  0.000000  0.000593  0.000165  0.000308  0.000000  0.000011  0.000011   \n",
       "\n",
       "            11       111       115    ...      zealand    zenith      zero  \\\n",
       "dems  0.000275  0.000000  0.000000    ...     0.000046  0.000000  0.000061   \n",
       "reps  0.000352  0.000011  0.000011    ...     0.000033  0.000011  0.000099   \n",
       "\n",
       "          zika  zimbabwe  zimbabwean       zip      zone     zones    zoning  \n",
       "dems  0.000031  0.000046    0.000000  0.000046  0.000061  0.000076  0.000000  \n",
       "reps  0.000011  0.000055    0.000011  0.000033  0.000044  0.000055  0.000055  \n",
       "\n",
       "[2 rows x 11513 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now see what the word proportions are.\n",
    "\n",
    "dtm_df_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "democrats      0.005979\n",
       "believe        0.003514\n",
       "work           0.003167\n",
       "americans      0.002815\n",
       "obama          0.002460\n",
       "jobs           0.002211\n",
       "make           0.002166\n",
       "workers        0.002150\n",
       "continue       0.002072\n",
       "democratic     0.002061\n",
       "need           0.002025\n",
       "committed      0.002021\n",
       "communities    0.001779\n",
       "new            0.001650\n",
       "help           0.001562\n",
       "al             0.001526\n",
       "health         0.001514\n",
       "gore           0.001479\n",
       "class          0.001476\n",
       "america        0.001464\n",
       "create         0.001408\n",
       "global         0.001406\n",
       "families       0.001358\n",
       "security       0.001330\n",
       "fight          0.001267\n",
       "build          0.001264\n",
       "clean          0.001163\n",
       "invest         0.001160\n",
       "good           0.001118\n",
       "pay            0.001103\n",
       "dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to turn these proportions into the most distinct for our two documents.\n",
    "# To do so, will subtract one row from the other to find the most unique words.\n",
    "\n",
    "# The words most distinct to the Democratic platforms.\n",
    "\n",
    "(dtm_df_prop.loc['dems']-dtm_df_prop.loc['reps']).sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "republican       -0.003855\n",
       "federal          -0.003156\n",
       "republicans      -0.003111\n",
       "congress         -0.002923\n",
       "government       -0.002533\n",
       "bush             -0.002364\n",
       "states           -0.002234\n",
       "current          -0.001903\n",
       "president        -0.001848\n",
       "state            -0.001473\n",
       "freedom          -0.001446\n",
       "free             -0.001285\n",
       "private          -0.001256\n",
       "law              -0.001250\n",
       "applaud          -0.001183\n",
       "religious        -0.001147\n",
       "united           -0.001015\n",
       "especially       -0.000929\n",
       "amendment        -0.000923\n",
       "legislation      -0.000903\n",
       "control          -0.000894\n",
       "individuals      -0.000866\n",
       "oppose           -0.000824\n",
       "foreign          -0.000813\n",
       "congressional    -0.000781\n",
       "urge             -0.000773\n",
       "affirm           -0.000763\n",
       "political        -0.000761\n",
       "encourage        -0.000752\n",
       "administration   -0.000751\n",
       "dtype: float64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The words most distinct to the Republican platforms.\n",
    "\n",
    "(dtm_df_prop.loc['dems']-dtm_df_prop.loc['reps']).sort_values(ascending=True)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dems    1.0\n",
       "reps    1.0\n",
       "Name: sum, dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also see the numbers associated with a specific word.\n",
    "# Subract one from the other to see the proportions we saw above.\n",
    "\n",
    "dtm_df_prop['federal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to discover the most distinctive words is to assign word scores which is weighted not just by the proportion of the word in its own text, but across all texts in a corpus. Therefore, a word frequent in both document is not as distinct.\n",
    "\n",
    "We do this by calculating tf-idf scores (see terminology above) calculated by:<br>\n",
    "number_of_documents / number_documents_with_term<br>\n",
    "\n",
    "Therefore:<br>\n",
    "tf_idf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "`scikit-learn` has this function built in so we do not have to calculate the tf-idf scores ourselves. The function is TfidfVectorizer similar to the previously used CountVectorizer. We imported this along with with our other libraries earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to define our tf-idf object similar to the count object earlier.\n",
    "\n",
    "tfidfvect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Then we will create the dtm again, but with tf-idf weights.\n",
    "\n",
    "dtm_tfidf_df = pandas.DataFrame(tfidfvect.fit_transform(corpus).toarray(), columns=tfidfvect.get_feature_names(), index = ['dems', 'reps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>104</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>111</th>\n",
       "      <th>115</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zero</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dems</th>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.022965</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.005529</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reps</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018231</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>0.009453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.002373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 11513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00       000        10       100      1000       104      10th  \\\n",
       "dems  0.000598  0.022965  0.005529  0.005529  0.000598  0.000000  0.000000   \n",
       "reps  0.000000  0.018231  0.005064  0.009453  0.000000  0.000475  0.000475   \n",
       "\n",
       "            11       111       115    ...      zealand    zenith      zero  \\\n",
       "dems  0.007655  0.000000  0.000000    ...     0.001276  0.000000  0.001701   \n",
       "reps  0.010804  0.000475  0.000475    ...     0.001013  0.000475  0.003039   \n",
       "\n",
       "          zika  zimbabwe  zimbabwean       zip      zone     zones    zoning  \n",
       "dems  0.000851  0.001276    0.000000  0.001276  0.001701  0.002126  0.000000  \n",
       "reps  0.000338  0.001688    0.000475  0.001013  0.001350  0.001688  0.002373  \n",
       "\n",
       "[2 rows x 11513 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see what this looks like. It appear similar to earlier dataframe, but with different proportion numbers.\n",
    "\n",
    "dtm_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "democrats      0.164963\n",
       "believe        0.093650\n",
       "work           0.080578\n",
       "americans      0.068515\n",
       "obama          0.068158\n",
       "jobs           0.057177\n",
       "workers        0.055713\n",
       "committed      0.054654\n",
       "democratic     0.054109\n",
       "continue       0.053898\n",
       "make           0.053782\n",
       "need           0.051126\n",
       "communities    0.045418\n",
       "al             0.041817\n",
       "gore           0.041090\n",
       "class          0.040454\n",
       "help           0.039065\n",
       "create         0.036695\n",
       "global         0.035747\n",
       "new            0.033513\n",
       "fight          0.033017\n",
       "build          0.032854\n",
       "invest         0.031847\n",
       "clean          0.031698\n",
       "families       0.030930\n",
       "good           0.028851\n",
       "pay            0.028663\n",
       "health         0.028588\n",
       "strengthen     0.028023\n",
       "security       0.027649\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now see what our tf-idf scores say about the most distinctive words. \n",
    "# This code is similar to the DTM code above.\n",
    "# We look at words most distinctive to the Democratic platforms first\n",
    "\n",
    "(dtm_tfidf_df.loc['dems']-dtm_tfidf_df.loc['reps']).sort_values(ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "republican       -0.120859\n",
       "federal          -0.103504\n",
       "republicans      -0.098064\n",
       "congress         -0.091850\n",
       "government       -0.089533\n",
       "states           -0.078211\n",
       "bush             -0.075343\n",
       "president        -0.070321\n",
       "current          -0.060378\n",
       "state            -0.049093\n",
       "freedom          -0.047071\n",
       "law              -0.043749\n",
       "free             -0.043117\n",
       "private          -0.041717\n",
       "united           -0.038379\n",
       "applaud          -0.036612\n",
       "religious        -0.036222\n",
       "administration   -0.030825\n",
       "especially       -0.030180\n",
       "nation           -0.029514\n",
       "amendment        -0.029145\n",
       "legislation      -0.028632\n",
       "control          -0.028457\n",
       "individuals      -0.027769\n",
       "oppose           -0.027366\n",
       "foreign          -0.026791\n",
       "political        -0.026288\n",
       "encourage        -0.025090\n",
       "national         -0.025082\n",
       "tax              -0.024722\n",
       "dtype: float64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now it is the Republican platforms turn.\n",
    "\n",
    "(dtm_tfidf_df.loc['dems']-dtm_tfidf_df.loc['reps']).sort_values(ascending=True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As we have seen the nltk and scikit-learn packages are very powerful tools for computational text analysis. Natural language tools allow us to easily tokenize our corpora, remove stopwords, count word frequency and ngrams. Concordance analysis also allows us to see the context surrounding specific words.\n",
    "\n",
    "By combining nltk and scikit-learn we can then look at distinctive words in our corpora and provides two different methods of seeing the words that most sets apart two sets of documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
